# -*- coding: utf-8 -*-
"""Copy_of_QuantumML2dPointSetRegistration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ap46mXtdBC-uz9_c8FFl7FTj9C2UPvT4

First import all necessay libraries
"""


# Commented out IPython magic to ensure Python compatibility.
# %pip install plyfile

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Tuple
import math
import random
import torch.nn as nn
import torch.nn.functional as F
from sklearn.neighbors import NearestNeighbors
from torch.utils.data import DataLoader
from torch.optim import Adam, AdamW
from scipy.linalg import block_diag
from scipy.ndimage.filters import gaussian_filter

from icp import *
from plyfile import PlyData
import glob
import sys
import os

BATCH_SIZE = 32
EPOCHS = 20
#EPOCHS = 0

TOTAL_RESULT_VECTOR = 9

#lamb2_choose = 5e-05
ls_ls = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]
#ind1 = int(sys.argv[-1])-1
lr_ =  0.000001 #ls_ls[ind1]

sigma = 0.0
"""The return matrix is a simple rotational matrix defined like [here](https://en.wikipedia.org/wiki/Rotation_matrix). The angle $\theta \in [0,2\pi]$ is mapped to the set of $\{0, \dots, 2^9 - 1\}$ Here $\pi$ will be mapped to 0. Every angle between $\pi$ and $0$ will have values with a leading 0 and every angle between $\pi$ and $2 \pi$ will have a leading 1"""

def map_angle_to_bit_and_rotMat2(angle, pot):
  k = (1/2)*np.pi / ((2**pot) - 1)
  x = int(math.floor(angle / k))
  #total = 2**9 
  #before = [0] * (total - x - 1)
  #after = [0] * (x)
  return_angle = x #np.array(before + [1] + after)
  return_mat = np.array([[np.cos(angle), -np.sin(angle)],[np.sin(angle), np.cos(angle)]])
  return (return_angle, return_mat)

"""The generated point could will be normally distributed around the origin with $m$ points"""

def get_pointcloud_from_image(fileName):
  ply_file = PlyData.read(fileName)
  data = ply_file.elements[0].data
  ls = []
  for n in data:
    ls.append(list(n)[:2])
  return np.array(ls)

def gen_random_image():
  size = np.random.randint(low = 20, high = 2000)
  #size = 30
  return np.random.uniform(low = -1, high = 1, size = (size,2))

def Qk(k):
  # here probably still a bug from the paper, as there are two 0.1 ? 
  omegals = [0.5, 0.2, 0.1, 0.1, 0.05]
  I = np.identity(2)
  M = np.array([[0,-1], [1,0]])
  Cls = [I,M,-I,-M]
  i = int(k / 5)
  j = k % 5
  return omegals[j] * Cls[i]

ls = [Qk(i) for i in range(20)]
ls = np.array(ls)
basis = block_diag(*ls)
Q_ls = ls

number_of_NN = 3

def phiN(yns, x_row):
  size1 = yns.shape[0] # number of points
  size2 = yns.shape[1] # number of NN
  size3 = yns.shape[2] # dimension of PC
  phi = np.zeros((len(Q_ls), size1 * size2 * size3))
  for i in range(size1):
    phi_n = np.zeros((len(Q_ls), size2 * 2))
    for j in range(size2):
      current_point = yns[i,j,:]
      ls = []
      for bMat in Q_ls:
        ls.append(-(bMat @ current_point).T)
      ls = np.array(ls)
      phi_n[:,2*j:2*(j+1)] += ls
      #print(phi_n)
      #print('-----------')
    phi[:,i*size2*size3:(i+1)*size2*size3] += phi_n
  mat = np.concatenate((x_row,phi))
  return mat

def genA(P,H): 
  x_row = np.zeros((1,P.shape[1]*number_of_NN*2))
  for i in range(P.shape[1]):
    for j in range(number_of_NN):
      x_row[0,i*2 * number_of_NN + j*2:i*2*number_of_NN+(j+1)*2] = P[:,i]
  nbrs = NearestNeighbors(n_neighbors=number_of_NN).fit(P.T)
  _, indices = nbrs.kneighbors(H.T)
  #indices = [[i] for i in range(91)]
  ybags = H.T[indices]
  phi = phiN(ybags,x_row)
  A = phi @ np.transpose(phi)
  return A

"""In order to generate the dataset we will randomly generate the point cloud and rotate it with a random angle n times. This product $H^T P$ will then be saved as a vector as well as the angle in bits"""

def gen_image_data(n, pot, from_, to_, myFiles, sigma):
  image_ls = []

  for filename in myFiles:
    x_coord = []
    y_coord = []
    with open(filename, 'r') as f: # open in readonly mode
      data = f.readlines()
      data = data[1:]
      read_hash = False
      for d in data:
        ls = d.split(' ')
        read_hash = '#' in ls or read_hash
        if not read_hash:
          x_coord.append(float(ls[0]))
          y_coord.append(float(ls[1]))
    img = np.array([x_coord, y_coord])
    image_ls.append(img.T) 
  
  ls = image_ls[from_:to_]
  for s in ls:
    P = s
    mean = P.mean(axis = 0)
    P -= mean
    scale = 1 / P.std(axis = 0) #1 / max(abs(P.min()), abs(P.max())) #
   
    P = scale * P
    number_of_rows = P.shape[0]
    #random_indices = np.random.choice(number_of_rows, size=30, replace=False)
    #P = P[random_indices, :]
    #print(random_indices)
    for _ in range(n):
      angle = (1/2)*np.pi*random.uniform(0,1) # just have here +- 15 degree
      angle = np.pi*random.uniform(0,1) # just have here +- 15 degree

      bin_ang, rot_mat = map_angle_to_bit_and_rotMat2(angle, pot)
      H = np.transpose(np.matmul(rot_mat, P.transpose()))
      H  = gaussian_filter(H, sigma=sigma)

      A = genA(P.T,H.T)
      dic['Input'].append(np.array(A.flatten(), dtype='f'))
      dic['Output'].append(bin_ang)
      dic['Image1'].append(P)
      dic['Image2'].append(H)
      dic['angle'].append(angle)


  df = pd.DataFrame(dic)
  df = df.sample(frac=1).reset_index(drop=True)
  return df

class Dataset(torch.utils.data.Dataset):
  'Characterizes a dataset for PyTorch'
  def __init__(self, dataFrame):
        'Initialization'
        self.df = dataFrame

  def __len__(self):
        'Denotes the total number of samples'
        return len(self.df.index)

  def __getitem__(self, index):
        'Generates one sample of data'
        # Select sample
        X = self.df['Input'][index]
        y = self.df['Output'][index]
        return X, y

class DatasetTest(torch.utils.data.Dataset):
  'Characterizes a dataset for PyTorch'
  def __init__(self, dataFrame):
        'Initialization'
        self.df = dataFrame

  def __len__(self):
        'Denotes the total number of samples'
        return len(self.df.index)

  def __getitem__(self, index):
        'Generates one sample of data'
        # Select sample
        X = self.df['Input'][index]
        y = self.df['Output'][index]
        X1 = self.df['Image1'][index]
        X2 = self.df['Image2'][index]
        return X, y, X1, X2

myFiles = glob.glob('*.txt')
dic = {'Input' : [], 'Output' : [], 'Image1' : [], 'Image2' : [], 'angle' : []}
random.shuffle(myFiles)


#df = gen_image_data(5000, 9, 0, 500, myFiles)
data_ls = [['1dataFrame1','1dataFrame2','1dataFrame3','1dataFrame4','1dataFrame5'],
  ['2dataFrame1','2dataFrame2','2dataFrame3','2dataFrame4','2dataFrame5'],
  ['3dataFrame1','3dataFrame2','3dataFrame3','3dataFrame4','3dataFrame5']]

#ls = data_ls[int(sys.argv[-1])-1]
#df_ls = []
#for f in ls:
#  df_ls.append(pd.read_pickle(f))

df_ls = []
for f in range(1,16):
  file = '60_deg_small_angle_data' + str(f)
  temp = pd.read_pickle(file)
  #print(file)
  #print(temp)
  df_ls.append(temp)

df = pd.concat(df_ls, ignore_index=True)
data_train = DataLoader(dataset = Dataset(df), batch_size = BATCH_SIZE, shuffle =True)
dic = {'Input' : [], 'Output' : []}
myFiles = glob.glob('*.txt')
dic = {'Input' : [], 'Output' : [], 'Image1' : [], 'Image2' : [], 'angle' : []}
random.shuffle(myFiles)


df2 = pd.read_pickle('eval_df')
data_test_1 = DataLoader(dataset = Dataset(df2[['Input', 'Output']]), batch_size = BATCH_SIZE, shuffle =True)

def dec2bin(x, bits):
    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)
    temp = x.unsqueeze(-1).bitwise_and(mask).ne(0).float()
    #temp[temp == 0] = -1
    return temp


def bin2dec(b, bits):
    #b[b == -1] = 0
    mask = 2 ** torch.arange(bits - 1, -1, -1).to(b.device, b.dtype)
    return torch.sum(mask * b, -1)

def gen_all_binary_vectors(length: int) -> torch.Tensor:
    result =  ((torch.arange(2**length).unsqueeze(1) >> torch.arange(length-1, -1, -1)) & 1).float()
    #result[result == 0] = -1
    return result

def bruteForceAngle(mat, x_gt, spectral_gap = False): 
  
    possible_x_trans = gen_all_binary_vectors(9)
    reshape_size = min(x_gt.shape[0], BATCH_SIZE)
    possible_x_trans = possible_x_trans.repeat(reshape_size,1,1)
    possible_x_trans = possible_x_trans.to(device)
    possible_x = torch.transpose(possible_x_trans,1,2)
    results = torch.matmul(possible_x_trans, mat)
    results = torch.matmul(results, possible_x)
    results = torch.diagonal(results, dim1 = 1, dim2 = 2)
    if not spectral_gap:
      results = torch.topk(results,2,largest=False,dim=1)
      result_value = results.values
      result_angle = results.indices

    else:
      re = torch.topk(results,2,largest=False,dim=1)
      result_value = results
      result_angle = re.values
    
    return result_value, result_angle

def mat_loss_func(A_mat, x_gt):
  (res_value, _) = bruteForceAngle(A_mat, x_gt)
  x_gt.type(torch.FloatTensor)
  reshape_size = min(x_gt.shape[0], BATCH_SIZE)
  x_gt = torch.reshape(x_gt, (reshape_size,9,1))
  x_gt_trans = torch.transpose(x_gt, 1,2)
  result = torch.matmul(x_gt_trans, A_mat)
  result = torch.matmul(result, x_gt)
  result = torch.abs(result - res_value)
  A_norm = torch.abs(A_mat).sum(dim = -1)
  A_norm = A_norm.sum(dim = -1)
  result = torch.flatten(result, start_dim=1) + A_norm
  result = result.mean()
  return result

def mat_loss_func3(A_mat, x_gt, lamb1, lamb2, network, input):
  A_mat = A_mat.to(device)
  (res_value, _) = bruteForceAngle(A_mat, x_gt)
  x_gt.type(torch.FloatTensor)
  reshape_size = min(x_gt.shape[0], BATCH_SIZE)
  x_gt = torch.reshape(x_gt, (reshape_size,9,1))
  x_gt_trans = torch.transpose(x_gt, 1,2)
  result = torch.matmul(x_gt_trans, A_mat)
  result1 = torch.matmul(result, x_gt)
  result = torch.abs(result1 - res_value[:,0])
  difference = torch.clone(torch.flatten(result, start_dim=1))
  difference[difference != 0] = -1
  difference[difference == 0] = 1
  difference[difference == -1] = 0
  #unique = torch.sum(A_mat, dim=1)
  #unique = torch.sum(unique, dim=1)
  #unique *= -0.5
  #unique += (1/(2**TOTAL_RESULT_VECTOR)) * res_value[:,0]
  #result = torch.flatten(result, start_dim=1) + lamb1 * network.sparse_loss(input) #+ lamb2 * unique
  #result = result.mean()
  result = torch.flatten(result, start_dim=1) + lamb1 * network.sparse_loss(input) - lamb2 * torch.abs(result1 - res_value[:,1])
  #result = result.sum()
  #result = result / (BATCH_SIZE - difference.sum() + 1)
  result = result.mean()
  return result, difference.detach().sum().item()


def mat_loss_func2(A_mat, x_gt, lamb1, lamb2, network, input):
  A_mat = A_mat.to(device)
  (res_value, fst_value) = bruteForceAngle(A_mat, x_gt, spectral_gap = True)
  x_gt.type(torch.FloatTensor)
  reshape_size = min(x_gt.shape[0], BATCH_SIZE)
  x_gt = torch.reshape(x_gt, (reshape_size,9,1))
  x_gt_trans = torch.transpose(x_gt, 1,2)
  result = torch.matmul(x_gt_trans, A_mat)
  result1 = torch.matmul(result, x_gt)
  result = torch.abs(result1 - fst_value[:,0])
  difference = torch.clone(torch.flatten(result, start_dim=1))
  difference[difference != 0] = -1
  difference[difference == 0] = 1
  difference[difference == -1] = 0
  #result = torch.flatten(result, start_dim=1) + lamb1 * network.sparse_loss(input) - lamb2 * torch.abs(result1 - res_value[:,1])
  #result = result.mean()
  result = torch.flatten(result, start_dim=1) + lamb1 * network.sparse_loss(input) - difference  * (result1 - (1/(2**TOTAL_RESULT_VECTOR)) * torch.sum(res_value, dim = 1))
  #result = result.sum()
  #result = result / (BATCH_SIZE - difference.sum() + 1)
  result = result.mean()
  return result, difference.detach().sum().item()

def binary(x, bits):
    mask = 2**torch.arange(bits-1,-1,-1).to(x.device, x.dtype)
    result = x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()
    result = result.float()
    #result[result == 0] = -1
    return result

class Network(nn.Module):

    def __init__(self):
        super().__init__()

        self.fc1 = nn.Linear(21*21, 79)
        self.fc2 = nn.Linear(79, 79)
        self.fc3 = nn.Linear(79+21*21, 79)
        self.fc4 = nn.Linear(79, 79)

        #self.fc5 = nn.Linear(697, 256)
        #self.fc6 = nn.Linear(256, 256)
        self.fc5 = nn.Linear(79, 9)

        #self.fc8 = nn.Linear(128, 256)
        #self.fc9 = nn.Linear(256, 256)
        #self.fc10 = nn.Linear(256, 512)


    def forward(self,x):
        input_x = torch.clone(x)
        #input_x = torch.repeat_interleave(input_x, 64, dim = 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = torch.cat((x, input_x), dim=1)
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x))

        #x = torch.reshape(x, (reshape_size, 9,1))
        #x = torch.diag_embed(x)
        #x_trans = torch.transpose(x,1,2)
        #x = x + x_trans
        #print(x.shape)
        #x = torch.sin(x)
        return x

    def sparse_loss(self,x):
        loss = 0 
        input_x = torch.clone(x)
        #input_x = torch.repeat_interleave(input_x, 64, dim = 1)
        x = F.relu(self.fc1(x))
        loss += torch.mean(torch.abs(x))
        x = F.relu(self.fc2(x))
        loss += torch.mean(torch.abs(x))
        x = torch.cat((x, input_x), dim=1)
        x = F.relu(self.fc3(x))
        loss += torch.mean(torch.abs(x))
        x = F.relu(self.fc4(x))
        loss += torch.mean(torch.abs(x))
        x = self.fc5(x)
        loss += torch.mean(torch.abs(x))

        #x = torch.cat((x, input_x), dim=1)
        #x = F.relu(self.fc5(x))
        #loss += torch.mean(torch.abs(x))

        #x = F.relu(self.fc6(x))
        #loss += torch.mean(torch.abs(x))



        #reshape_size = min(x.shape[0], BATCH_SIZE)

        #x = torch.sin(x)
        #loss += torch.mean(torch.abs(x))

        return loss

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

net = Network()
criterion = nn.L1Loss().to(device)
optm = Adam(net.parameters(), lr=lr_)

#device = torch.device("cpu")
net.to(device)
#criterion#.to(device)

difference_stat = []
diff_temp = 0
for epoch in range(EPOCHS):  # loop over the dataset multiple times
    '''if epoch == 2:
      for g in optm.param_groups:
        g['lr'] = 0.001'''
    #if epoch > 1:
    #  criterion = mat_loss_func
    running_loss = 0.0
    for i, data in enumerate(data_train, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data[0].to(device), data[1].to(device)
        # zero the parameter gradients
        optm.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        labels = dec2bin(labels, 9)

        loss= criterion( outputs,labels) + 10e-4 * net.sparse_loss(inputs)
        #loss = criterion(outputs, labels,10e-4, 10e-3, net, inputs)

        loss.backward()
        optm.step()

        '''if epoch == 3 and not already_changed:
          for g in optm.param_groups:
            g['lr'] = 0.0001
          already_changed = True'''

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

#torch.save(net, 'siren_Adamtrad_9_full' + str(lr_)+ 'and' + str(lamb2_choose)+ '.pt')
torch.save(net, 'pure_5_2.pt')
print('Finished Training')

def argmax(iterable):
    return max(enumerate(iterable), key=lambda x: x[1])[0]


def ana(df, diff):
  correct = 0
  total = 0
  histogram_icp = np.zeros(256)
  zero = 0

  histogram = np.zeros(256)
  A_distripution = np.array([])
  with torch.no_grad():
      counter = 0
      for data in df:
          input, labels = data[0].to(device), data[1].to(device)
          outputs = net(input)
          predicted = outputs.data
          conversion = predicted.cpu()
          labels = binary(labels, 9)
          res_angle = torch.round(outputs)
          #(_, res_angle) =  bruteForceAngle(predicted,labels)
          #res_angle = res_angle[:,0]
          A_save = conversion
          A_save = A_save.numpy().flatten()
          A_distripution = np.append(A_distripution, A_save)
          X_save = res_angle.cpu()

          res_angle =  bin2dec(res_angle, 9)
          labels = bin2dec(labels, 9)

          lables_numpy = labels.cpu().numpy()
          #total += labels.size(0) 
          #print(res_angle)
          #print(lables_numpy)
          #correct += (resu == lables_numpy).sum()
          res_angle = res_angle.cpu().numpy()
          print(res_angle.shape)
          print(lables_numpy.shape)
          #correct += np.count_nonzero((res_angle == lables_numpy).sum(axis=0))
          #zero += np.count_nonzero(res_angle==0)
          #counter +=1

          hist = (np.abs(res_angle - lables_numpy) + 256) % 256
          hist = hist.astype(int)
          for h in hist:
            histogram[h] += 1

  np.save('pure_5_2_' + str(diff), histogram)
  np.save('icp_3_' + str(lr_), histogram_icp)

ana(data_test_1, 0.1)
